#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\begin_preamble
\usepackage{url} 
\end_preamble
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman times
\font_sans helvet
\font_typewriter courier
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks true
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry false
\use_amsmath 2
\use_esint 0
\use_mhchem 0
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Language Learning Diary
\end_layout

\begin_layout Author
Linas Vepstas
\end_layout

\begin_layout Abstract
Collection of thoughts and notes, pursuant to language learning.
 Organized in mostly chronological order.
\end_layout

\begin_layout Section*
31 December 2013
\end_layout

\begin_layout Standard
Easier said than done.
 Seems 'obvious' but its not.
 Goal: constraint set of link rules to model only the observed language,
 via link-grammar.
 Definitions.
 Let 
\begin_inset Formula $P(R(w_{l},w_{r}))$
\end_inset

 represent the probability (frequency) of observing two words, 
\begin_inset Formula $w_{l}$
\end_inset

 and 
\begin_inset Formula $w_{r}$
\end_inset

 in some relationship or pattern 
\begin_inset Formula $R$
\end_inset

.
 Typically, its a (link-grammar) linkage of type 
\begin_inset Formula $t$
\end_inset

 connecting word 
\begin_inset Formula $w_{l}$
\end_inset

 on the left to word 
\begin_inset Formula $w_{r}$
\end_inset

 on the right; however, 
\begin_inset Formula $R$
\end_inset

 can be more general than that.
\end_layout

\begin_layout Standard
The simplest model has only one type 
\begin_inset Formula $t$
\end_inset

, the ANY type, and assigns equal probabilities to all words.
 But we know all words are not equi-probable, so let 
\begin_inset Formula $P(w)$
\end_inset

 be the probability of observing word 
\begin_inset Formula $w$
\end_inset

.
 We know from experiece this is a Zipfian distribution.
 We are then interested in the conditional probability 
\begin_inset Formula $P(R(w_{l},w_{r})|w_{l},w_{r})$
\end_inset

 of observing the two words 
\begin_inset Formula $w_{l}$
\end_inset

 and 
\begin_inset Formula $w_{r}$
\end_inset

 in a relation 
\begin_inset Formula $R=R(w_{l},w_{r})$
\end_inset

, given that the two individual words were observed.
 From the definition of conditional probabilities, we get 
\begin_inset Formula 
\[
P(R)=P(R|w_{l},w_{r})P(w_{l})P(w_{r})
\]

\end_inset

Or that 
\begin_inset Formula 
\[
P(R|w_{l},w_{r})=\frac{P(R)}{P(w_{l})P(w_{r})}
\]

\end_inset

Here, the relation 
\begin_inset Formula $R$
\end_inset

 encompasses several facts: that one word is to the left of the other, and
 that they are connected by a certain link-type, as well as capturing other
 'ambient' information, perhaps such as other nearby words.
\end_layout

\begin_layout Standard
We need to harmonize here a little bit with the Deniz Yuret concepts and
 notation.
 He defines a probability 
\begin_inset Formula $P(w_{l},w_{r})$
\end_inset

 of seeing the ordered pair; that is, the relation 
\begin_inset Formula $R$
\end_inset

 is implicit.
 To make it explicit, we should write: 
\begin_inset Formula $P(w_{l},w_{r})=P(R(w_{l},w_{r}))$
\end_inset

 to indicate the relation explicitly, and to note that the order of the
 positions in the relation matter.
 Yuret also uses the notation 
\begin_inset Formula $P(w_{l},*)$
\end_inset

 and 
\begin_inset Formula $P(*,w_{r})$
\end_inset

 for wild-card summations, defined as 
\begin_inset Formula 
\[
P(w_{l},*)=\sum_{w_{r}}P(w_{l},w_{r})\qquad\mbox{and}\qquad P(*,w_{r})=\sum_{w_{l}}P(w_{l},w_{r})
\]

\end_inset

In practical use, one quickly observes that 
\begin_inset Formula $P(w_{l},*)$
\end_inset

 is almost equal to 
\begin_inset Formula $P(w_{l})$
\end_inset

 but not quite, since 
\begin_inset Formula $P(w_{l},*)$
\end_inset

 is the probability of seeing 
\begin_inset Formula $w_{l}$
\end_inset

 within the certain relationship or pattern, which must be less than the
 prior probability of observing 
\begin_inset Formula $w_{l}$
\end_inset

 in general.
 Thus, one has 
\begin_inset Formula $P(w_{l},*)\le P(w_{l})$
\end_inset

 which can be viewed as a conditional probability:
\begin_inset Formula 
\[
P(w_{l},*)=P(R(w_{l},*))=P(R(w_{l},*)|w_{l})P(w_{l})
\]

\end_inset

In practice, then, for word-pairs, one has that 
\begin_inset Formula $P(R|w_{l})$
\end_inset

 is almost equal to 1, but not quite.
 Inserting this into the above gives 
\begin_inset Formula 
\[
P(R(w_{l},w_{r})|w_{l},w_{r})=\frac{P(R(w_{l},w_{r}))\ P(R(w_{l},*)|w_{l})\ P(R(*,w_{r})|w_{r})}{P(w_{l},*)P(*,w_{r})}
\]

\end_inset

Re-ordering this gives
\begin_inset Formula 
\begin{equation}
\frac{P(R(w_{l},w_{r})|w_{l},w_{r})}{P(R(w_{l},*)|w_{l})\ P(R(*,w_{r})|w_{r})}=\frac{P(w_{l},w_{r})}{P(w_{l},*)P(*,w_{r})}\label{eq:basic pair}
\end{equation}

\end_inset

The right hand side above is recognizable from Yuret's work; he defines
 the mutual information as 
\begin_inset Formula 
\begin{equation}
\mbox{MI}(w_{l},w_{r})=\log_{2}\,\frac{P(w_{l},w_{r})}{P(w_{l},*)P(*,w_{r})}\label{eq:Yuret's MI}
\end{equation}

\end_inset

so that laarge positive MI is associated with words that occur together
 only with themselves (e.g.
 '
\emph on
Northern Ireland
\emph default
' from his examples.) So, on the right, we have that 
\begin_inset Formula $P(w_{l},w_{r})$
\end_inset

 is usually very small, and that 
\begin_inset Formula $P(w,*)\approx P(*,w)\approx P(w)$
\end_inset

 subject to the inequality given before.
 
\end_layout

\begin_layout Standard
The LHS of eqn 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:basic pair"

\end_inset

 is a stranger to me.
 First, we have observationally seen that 
\begin_inset Formula $P(R(w_{l},*)|w_{l})\approx P(R(*,w_{r})|w_{r})\approx1$
\end_inset

, and thus must conclude that 
\begin_inset Formula $P(R(w_{l},w_{r})|w_{l},w_{r})$
\end_inset

 is 'large'; much larger than (unconditional) word-pair frequencies.
 Anyway, as an equation, its kind-of nicer, since it clearly isolates the
 role of the relation, since, by defintion, we have 
\begin_inset Formula $P(R(w_{l},w_{r})|w_{l},w_{r})=P(R(w_{l},w_{r})|w_{r},w_{l})$
\end_inset

 so that it functions like a real conditional probability, rather than this
 implicit-word-order thingy that Yuret uses.
 The Yuret notation is very convenient, but sometimes obscures things.
 OK, I think I'm done with that.
\end_layout

\begin_layout Subsubsection*
Update 14 Jan 2014
\end_layout

\begin_layout Standard
OK, The LHS of equation 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:basic pair"

\end_inset

 then demonstrates how to obtain conditional entropies in general.
 Thus, given an 
\begin_inset Formula $n$
\end_inset

-point relation 
\begin_inset Formula $R(x_{1},x_{2,}\cdots,x_{n})$
\end_inset

 one computes first the unconditional probability 
\begin_inset Formula $P(R(x_{1},x_{2,}\cdots,x_{n}))$
\end_inset

.
 The conditional probability is then obtained as usual:
\begin_inset Formula 
\[
P(R(x_{1},x_{2,}\cdots,x_{n})\,|\, x_{1},x_{2,}\cdots,x_{n})=\frac{P(R(x_{1},x_{2,}\cdots,x_{n}))}{P(x_{1})P(x_{2})\cdots P(x_{n})}
\]

\end_inset

The entropy is then build recursively by normalizing by the probability
 of wild-card relations:
\begin_inset Formula 
\[
MI(R(x_{1},x_{2,}\cdots,x_{n})\,|\, x_{1},x_{2,}\cdots,x_{n}))=\log_{2}\frac{P(R(x_{1},x_{2,}\cdots,x_{n})\,|\, x_{1},x_{2,}\cdots,x_{n}))}{P(R(*,x_{2,}\cdots,x_{n})\,|\, x_{2,}\cdots,x_{n}))\, P(R(x_{1},*,\cdots,x_{n})\,|\, x_{1},x_{3,}\cdots,x_{n}))\cdots}
\]

\end_inset

(Right? We're not dividing by multiple *'s here, yes?) 
\end_layout

\begin_layout Section*
1 January 2014
\end_layout

\begin_layout Standard
OK, after that side distraction, which helped clear up notation, back to
 the main show ...
\end_layout

\begin_layout Standard
The main show is this: We want to model language, and specifically, find
 a 'minimal' set of relations R that are accurately generative.
 The meaning of 'minimal' seems obvious, intuitively, but a lot harder to
 pin down mathematically.
 We need to pin it down to get an algorithm that works in a trust-worthy,
 understandable fashion.
\end_layout

\begin_layout Standard
So: what is the total space of relations, and how do we find it? The simplest
 model is then a Zipfian distribution of words, but placed in random order.
 This model has a total entropy of 
\begin_inset Formula 
\[
H=-\sum_{w}P(w)\log_{2}P(w)
\]

\end_inset

For a recent swipe at parsing a few hundred articles from the French wikipedia,
 I get H=7.2.
 This is on 17K words, observed a total of 35M times (actually, observerd
 each sentence 100 times, so really just 350K 'true' observations of words).
\end_layout

\begin_layout Standard
How does one count the entropy of the rule-set? Elucidating this is the
 goal-set.
\end_layout

\begin_layout Standard
But first, step back: describe the rules.
 
\end_layout

\begin_layout Standard
OK ...
 so, once again ...
 sentence structure is to be described via link-grammar, using disjoined
 conjunctions of connectors.
 This is theoretically sound, as it seems to be isomorphic to categorical
 grammars (via type-theory of the connectors; need a formal proof of this
 someday, but for now it seems 'obvious').
 Also link-grammar is fully compatible with dependency grammar.
 So lets move forward.
 
\end_layout

\begin_layout Subsection*
How to count relations
\end_layout

\begin_layout Standard
Consider a sentence with 
\begin_inset Formula $n$
\end_inset

 words in it, numbered 
\begin_inset Formula $w_{1},w_{2},\cdots,w_{n}$
\end_inset

 left to right.
 We want to constrain grammar by discovering a set of relations 
\begin_inset Formula $R(w_{1},w_{2},\cdots,w_{n})$
\end_inset

 such that 
\begin_inset Formula $P(R(w_{1},w_{2},\cdots,w_{n}))>0$
\end_inset

 when the sentence is gramatically valid (i.e.
 such an 
\begin_inset Formula $R$
\end_inset

 exists), and 
\begin_inset Formula $P$
\end_inset

 is zero when no such 
\begin_inset Formula $R$
\end_inset

 exists (i.e.
 the sentence is not gramatically valid.) The first and most obvious simplificati
on rule is to observe that 
\begin_inset Formula $R$
\end_inset

 can be replaced by 
\begin_inset Formula $R(W_{1},W_{2},\cdots,W_{n})$
\end_inset

 where each 
\begin_inset Formula $W_{k}$
\end_inset

 is a set of words.
 That is, instead of listing each sentences individually, we list certain
 classes of sentences.
 In other words, the relations 
\begin_inset Formula $R(w_{1},w_{2},\cdots,w_{n})$
\end_inset

 are in one-to-one correspondance with a list of grammatical sentences 
\begin_inset Formula $(w_{1},w_{2},\cdots,w_{n})$
\end_inset

, so simply listing all possible sentences is a very verbose way of specifying
 a grammar.
 It is linguistically 'obvious' that sentences fall into classes, and so
 the two relations 
\begin_inset Formula $R('this','is','a','dog')$
\end_inset

 and 
\begin_inset Formula $R('this','is','a','cat')$
\end_inset

 can be replaced by 
\begin_inset Formula $R('this','is','a',W_{n})$
\end_inset

 where 
\begin_inset Formula $W_{n}=\{'dog','cat'\}$
\end_inset

.
 In fact, 
\begin_inset Formula $W_{n}$
\end_inset

 can be a rather large set of nouns.
 
\end_layout

\begin_layout Standard
So ...
 the question is: what is the reduction of complexity, by performing this
 classification? What is the correct way of counting? I assume that 'complexity'
 is a synonym for 'entropy', so we are looking to do two things: enumerate
 the states of the system, and proivde a measure for complexity.
 So, lets consider a language with 
\begin_inset Formula $N$
\end_inset

 nouns, so that the cardinality of 
\begin_inset Formula $W_{n}$
\end_inset

 is 
\begin_inset Formula $|W_{n}|=N$
\end_inset

 and the only valid sentences are 
\begin_inset Formula $('this','is','a',w)$
\end_inset

 with 
\begin_inset Formula $w\in W_{n}$
\end_inset

 .
 Before simplification, we had 
\begin_inset Formula $N$
\end_inset

 relations 
\begin_inset Formula $R$
\end_inset

, one per sentence.
 We also had 
\begin_inset Formula $N+3$
\end_inset

 sets, each set containing a single word; the 
\begin_inset Formula $N$
\end_inset

 nouns, and the three words 
\begin_inset Formula $'this','is','a'$
\end_inset

.
 After simplification, we have one relation 
\begin_inset Formula $R$
\end_inset

, and four sets; three of the sets have cardinality 1, the fourth set has
 cardinality 
\begin_inset Formula $N$
\end_inset

.
 
\end_layout

\begin_layout Standard
I think the correct counting rule is to count set-membership relations on
 equal footing with structural relations.
 Thus, before simplification, we had 
\begin_inset Formula $N+3$
\end_inset

 sets, each a singleton, and thus 
\begin_inset Formula $N+3$
\end_inset

 set membership relations.
 After simplification, we have four sets, but still have 
\begin_inset Formula $N+3$
\end_inset

 set membership relations.
 Thus, this particular simplification step does not reduce the number of
 membership relations at all.
 I think this is the right way to count.
 Let provisionally go with this and see what happens.
 Thus, before simplification, we had 
\begin_inset Formula $2N+3$
\end_inset

 relations grand-total, and afterwords, we have 
\begin_inset Formula $N+4$
\end_inset

 relations grand-total.
\end_layout

\begin_layout Standard
What is the correct 'thermodynamic' picture of what's going on? In this
 toy problem, we have a grand-total state space of size 
\begin_inset Formula $(N+3)^{4}$
\end_inset

 since any of the 
\begin_inset Formula $N+3$
\end_inset

 words can appear in any of the four slots in a four-word sentence (micro-canonn
ical ensemble).
 The entropy, at 'infinite temperature' where all possible four-word sequences
 occur with equal probability is then 
\begin_inset Formula $4\log_{2}(N+3)$
\end_inset

.
 The entropy of the set of grammatical sentences is 
\begin_inset Formula $\log_{2}N$
\end_inset

 since there are only 
\begin_inset Formula $N$
\end_inset

 possible grammatical sentences.
 In this toy grammar, there are also invalid setences of length 1,2,3,5,6,7,...
 and so the total size of the space of word-sequences is clearly infinite.
 
\end_layout

\begin_layout Standard
OK, so the space of word-sequences is very concrete, and easy to describe
 and measure, at least for toy grammars.
 What about the space of relations? Well, the claim is that the entropies
 of the before-and-after models are 
\begin_inset Formula $\log_{2}(2N+3)$
\end_inset

 and 
\begin_inset Formula $\log_{2}(N+4)$
\end_inset

, respectively.
 Neither of these matches the entropy of the set of allowed sentences, so
 this seems paradoxical, and begs the questions 'did we count correctly?'
 and 'did we actually simplify anything by making the above change of descritpio
n?' Hmm.
 The correct answer seems to be 'no' and 'no'.
 The correct counting methodology seems to be to subtract 1 from the cardinality
 of every set.
 This would then give both 
\begin_inset Formula $\log_{2}N$
\end_inset

 as the entropy for both the before and after relation-sets.
 Thus, before, we had 
\begin_inset Formula $N$
\end_inset

 relations and 
\begin_inset Formula $N+3$
\end_inset

 sets, each of weight zero, for a total wieghted-relation count of 
\begin_inset Formula $N$
\end_inset

.
 After, we have one relation and four sets; three of the sets have wieght
 zero, one set has a weight of 
\begin_inset Formula $N-1$
\end_inset

 so the total wieghted relations is again 
\begin_inset Formula $N$
\end_inset

.
 This seems to resolve the paradox.
 Lets move on to more challenging domains.
 
\end_layout

\begin_layout Subsection*
Counting Link-Grammar Relations
\end_layout

\begin_layout Standard
Per link-grammar, each relation is decomposable into pair-wise relations;
 this is the so-called 'parse' of a sentence.
 If the relation is a single word-per-slot sentence relation, then the 'parse'
 is literal.
 We write 
\begin_inset Formula 
\begin{equation}
R(w_{1},w_{2},\cdots,w_{n})=\prod_{j,k,m}R_{\alpha}(w_{j},w_{k},t_{m})\ Q(R_{\alpha},R_{\beta},\cdots,R_{\omega})\label{eq:pair-decompose}
\end{equation}

\end_inset

where 
\begin_inset Formula $R_{\alpha}(w_{j},w_{k},t_{m})$
\end_inset

 is a single connected pair of words, connected by the connector 
\begin_inset Formula $t_{m}$
\end_inset

.
 The product symbol 
\begin_inset Formula $\prod$
\end_inset

 implies that all such binary relations must hold.
 The awkward 
\begin_inset Formula $Q(R_{\alpha},R_{\beta},\cdots,R_{\omega})$
\end_inset

 at the end is the additional no-links-cross constraint in the current link-gram
mar parser.
 Its a non-local constraint involving all of the binary relations.
 It also subsumes any 'post-processing' rules, although, for the language
 learnign exercise, there won't be any post-processing rules.
 At any rate, 
\begin_inset Formula $Q$
\end_inset

 is a place where higher ordrer constraints can be applied.
 In particular, the most genneral form for 
\begin_inset Formula $Q$
\end_inset

 should be 
\begin_inset Formula $Q(R_{\alpha},R_{\beta},\cdots,R_{\omega},w_{1},w_{2},\cdots,w_{n})$
\end_inset

 since, in principle, it could depend on the word-choice, although the no-links-
cross constraint does not.
 
\end_layout

\begin_layout Standard
Yuret proposes a way of discovering the pair-wise relations
\begin_inset CommandInset citation
LatexCommand cite
key "Yuret1998"

\end_inset

.
 He makes the implicit, unvoiced assumption that there is a single, unique
 connector type 
\begin_inset Formula $t_{m}$
\end_inset

 for every ordered pair of words 
\begin_inset Formula $w_{j},w_{k}$
\end_inset

; that is, that 
\begin_inset Formula $t_{m}=t_{m}(w_{j},w_{k})$
\end_inset

.
 Viz, specifically, that such connectors are in 1-1 correspondance with
 word-pairs.
 (I don't think he's aware of this assumption; I don't think anyone has
 ever before realized that he's making such an assumption; certainly, I
 haven't).
 Yuret then makes two claims: first, that the only possible grammatically
 correct parses are those of the above form (eqn 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:pair-decompose"

\end_inset

) for which the relations 
\begin_inset Formula $R_{\alpha}(w_{j},w_{k},t_{m}(w_{j},w_{k}))$
\end_inset

 have been previously observed; secondly, that there is a natural ranking
 of such allowed parses by summing the total mutual information associated
 with each word-pair.
\end_layout

\begin_layout Standard
These two concepts give rise to the idea of minimum-spanning-tree parsers.
 Such parsers work in a two-step process: a training phase, and a parse
 phase.
 In the training phase, one gathers a lot of statistics about mutual information.
 The important point here is that this is unsupervised training.
 To parse, one first creates a graph clique, with every word connected to
 every other.
 One uses the gathered MI to define graph edge lengths.
 Finally, the corrrect parse is then the maximum spanning tree of the graph
 (maximizing the MI, summed over the tree edges in the graph).
\end_layout

\begin_layout Standard
Here, we use the same idea, but then take the next step.
 The spanning tree can be decomposed into a set of link-grammar disjuncts,
 one disjunct per word.
 The disjunct is merely a list of the connections that one word makes.
 It consists of the type, and the direction.
 The direction is left or right.
 The type is the 
\begin_inset Formula $t_{m}=t_{m}(w_{j},w_{k})$
\end_inset

 defined above.
 By parsing a large number of sentences, we can now automatically discover
 a large number of disjuncts, in an unsupervised manner.
\end_layout

\begin_layout Standard
The goal, the next step, is then to reduce the total number of disjuncts,
 and the total number of types, by clustering and discovering similarities.
\end_layout

\begin_layout Section*
3 January 2014
\end_layout

\begin_layout Subsection*
No-crossing Minimum Spanning Trees
\end_layout

\begin_layout Standard
It turns out that writing an algorithm for a no-crossing minimum spanning
 tree is surprisingly painful; enforcing the no-crossing constraint requies
 treatment of a number of special cases.
 But perhaps this is not actually rquired! R.
 Ferrer i Cancho in 
\begin_inset Quotes eld
\end_inset

Why do syntactic links not cross?
\begin_inset Quotes erd
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "Ferrer2006"

\end_inset

 shows that, when attempting to arrange a random set of points on a line,
 in such a way as to minimize euclidean distances between connected points,
 one ends up with trees that almost never cross!
\end_layout

\begin_layout Standard
Other related references:
\end_layout

\begin_layout Itemize
Crossings are rare: Havelka, J.
 (2007).
 Beyond projectivity: multilingual evaluation of constraints and measures
 on non-projective structures.
 In: Proceedings of the 45th Annual Meeting of the Association of Computational
 Linguistics (ACL-07): 608-615.
 Prague, Czech Republic: Association for Computational Linguistics.
 
\end_layout

\begin_layout Itemize
Hubbiness is a better model of sentence complexity than mean dependency
 distance: Ramon Ferrer-i-Cancho (2013) 
\begin_inset Quotes eld
\end_inset

Hubiness, length, crossings and their relationships in dependency trees
\begin_inset Quotes erd
\end_inset

, ArXiv 1304.4086 --- also states: maximum number of crossings is bounded
 above by mean dependency length.
 Also, mean dependency length is bounded below by variance of degrees of
 vertexes (i.e.
 variance in number of connectors a word can have).
\end_layout

\begin_layout Itemize
Languages tends to be close to the theoretical minimum possible dependency
 distance, if it was legal to re-arrange words arbitrarily.
 See Temperley, D.
 (2008).
 Dependency length minimization in natural and artificial languages.
 Journal of Quantitative Linguistics, 15(3):256-282.
 
\end_layout

\begin_layout Itemize
Park, Y.
 A.
 and Levy, R.
 (2009).
 Minimal-length linearizations for mildly context-sensitive dependency trees.
 In Proceedings of the North American Chapter of the Association for Computation
al Linguistics - Human Language Technologies (NAACL-HLT) conference.
 
\end_layout

\begin_layout Itemize
Sentences with long dependencies are hard to understand: The original claim
 is from Yngve, 1960, having to do with phrase-structure depth.
 See -- Gibson, E.
 (2000).
 The dependency locality theory: A distance-based theory of linguistic complexit
y.
 In Marantz, A., Miyashita, Y., and O'Neil, W., editors, Image, Language, Brain.
 Papers from the first Mind Articulation Project Symposium.
 MIT Press, Cambridge, MA.
 
\end_layout

\begin_layout Itemize
(Cite this, its good) Mean dependency distance is a good measure of sentence
 complexity -- for 20 languages -- Haitao Liu gives overview starting from
 Yngve.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Liu2008"

\end_inset

.
 
\end_layout

\begin_layout Itemize
Sentences with long dependencies are rarely spoken: Hawkins, J.
 A.
 (1994).
 A Performance Theory of Order and Constituency.
 Cambridge University Press, Cambridge, UK.
 ----Hawkins, J.
 A.
 (2004).
 Efficiency and Complexity in Grammars.
 Oxford University Press, Oxford, UK.
 ----Wasow, T.
 (2002).
 Postverbal Behavior.
 CSLI Publications, Stanford, CA.
 Distributed by University of Chicago Press.
 
\end_layout

\begin_layout Standard
So, ratherr than imposing no-crossing as a constraint on the parser, instead,
 let it find its own way into the grammar.
 Just implement a plain-old MST parser, punt on crossing.
\end_layout

\begin_layout Section*
11 January 2014
\end_layout

\begin_layout Subsection*
Clustering Redux
\end_layout

\begin_layout Standard
OK, so what is the very next algorithmic step? Up to here, we've generated
 a large number of unique disjuncts.
 Now what?
\end_layout

\begin_layout Standard
Back to counting.
 Lets do the French dictionary.
 The database fr_pairs contains table atoms_mi_snapshot.
 So:
\end_layout

\begin_layout Itemize

\family typewriter
select count(*) from atoms_mi_snapshot;
\family default
 returns 415532
\end_layout

\begin_layout Section*
15 January 2014
\end_layout

\begin_layout Subsection*
Embodied Learning
\end_layout

\begin_layout Standard
OK, so maybe learning syntax before emantics puts the cart before the horse.
 Can we learn a world-model first, and then gradually annotate and correct
 it as our linguistic comprehension improves? So, for example, can we start
 with a world-model obtained via document summarization? How do we annotate
 this model with newly discovered data?
\end_layout

\begin_layout Standard
Related question: how to automatically discover ontologies? Automated, unsupervi
sed concept, entity extraction? Semantic context change over time?
\end_layout

\begin_layout Standard
Steps: 
\end_layout

\begin_layout Enumerate
How do I extract entities out of a text? The extraction doesn't have to
 be perfect; having candidate entities is enough.
 How do I put a confidence rating on the entiy, and how do I discard the
 low-cnfidence ones?
\end_layout

\begin_layout Enumerate
Once entities are extracted, I want to start decorating them with attributes
 (adjectives, modifiers), to build a network.
\end_layout

\begin_layout Enumerate
Once a network is built, it needs to be factually reconciled, using logical
 reasoning and an ontology (is-a and has-a relations).
 Need to do this so that upon reading 
\begin_inset Quotes eld
\end_inset

colorless green ideas
\begin_inset Quotes erd
\end_inset

, we can deduce that ideas are either colorless or green, but not both.
\end_layout

\begin_layout Enumerate
How to automatically extract an ontolofy from free text?
\end_layout

\begin_layout Standard
The above seem to be the central steps/core issues for creating a world-model,
 unsupervised, from text.
\end_layout

\begin_layout Subsection*
Entropy
\end_layout

\begin_layout Standard
Some refresher notes: 
\end_layout

\begin_layout Itemize
\begin_inset Quotes eld
\end_inset

The Boltzmann distribution is the so-called canonical distribution, meaning
 it maximizes entropy subject to a constraint on the expected value of energy.
\begin_inset Quotes erd
\end_inset

 (viz, this is the MaxEnt principle.
 except for MaxEnt, the constraint is not on energy but a set of features.)
\end_layout

\begin_layout Itemize
Define 
\begin_inset Quotes eld
\end_inset

Shanon Entropy
\begin_inset Quotes erd
\end_inset

 as 
\begin_inset Formula $S_{s}=-k_{B}\sum p\log p$
\end_inset


\end_layout

\begin_layout Itemize
The 
\begin_inset Quotes eld
\end_inset

Boltzmann Entropy
\begin_inset Quotes erd
\end_inset

 
\begin_inset Formula $S_{B}$
\end_inset

 is the shanon entrpy of the microcanonical esemble: it maximizes the entropy
 (MaxEnt) for a fixed value of the energy.
 (MaxEnt: not the energy, but for a fixed set of features).
 (viz, 
\begin_inset Formula $S_{B}=k_{B}\log\left(\epsilon\frac{d\Omega}{dE}\right)$
\end_inset

 with 
\begin_inset Formula $\Omega$
\end_inset

 being number of states, E the energy, 
\begin_inset Formula $\epsilon$
\end_inset

 a constant of dimension energy to make arg of the log dimensionless.)
\end_layout

\begin_layout Itemize
The 
\begin_inset Quotes eld
\end_inset

Gibbs Entropy
\begin_inset Quotes erd
\end_inset

 is the shanon entropy, maximized for a system held to constraint that energy
 is less-than-or-equal to E.
 (!) This gives 
\begin_inset Formula $S_{G}=k_{B}\log\Omega$
\end_inset

 (duhh, take 
\begin_inset Formula $p=1/\Omega$
\end_inset

 for 
\begin_inset Formula $\Omega$
\end_inset

 states.
 For a non-sharp cutoff, the Shannon entropy is primal.).
 
\end_layout

\begin_layout Itemize
Gibbs and Boltzmann entropies give different results for N-particle systems
 when N is very small.
 Viz, an off-by-one error for N.
 In some ways, 
\begin_inset Formula $S_{G}$
\end_inset

 is more correct (at low temp, quantum systems).
 See Jörn Dunkel and Stefan Hilbert (2014) 
\begin_inset Quotes eld
\end_inset

Consistent thermostatistics forbids negative absolute temperatures
\begin_inset Quotes erd
\end_inset

 Nature Physics DOI: 10.1038/NPHYS2815 
\end_layout

\begin_layout Subsection*
Why does Yuret's MST work?
\end_layout

\begin_layout Standard
There is an interesting simplification that happens with minimum-spanning
 tree parsers driven by entropy.
 If we use Yuret's definition of the MI of word-pairs, then, Yuret says
 (I should re-read his stuff) that we should maximize the entropy 
\begin_inset Formula 
\begin{equation}
\sum_{w_{l,}w_{r}}MI(w_{l},w_{r})\label{eq:MST entropy}
\end{equation}

\end_inset

Wy? Why this, instead of the maybe 
\begin_inset Quotes eld
\end_inset

more obviously correct
\begin_inset Quotes erd
\end_inset

 sum: 
\begin_inset Formula 
\begin{equation}
\sum_{w_{l,}w_{r}}P(w_{l},w_{r})MI(w_{l},w_{r})\label{eq:wrong MST entropy}
\end{equation}

\end_inset

I think I can hand-wave the answer.
 The answer is that we don't really know the probability of 
\begin_inset Formula $P(w_{l},w_{r})$
\end_inset

 
\emph on
for the given sentence! 
\emph default
We know 
\begin_inset Formula $P(w_{l},w_{r})$
\end_inset

 for a large corpus, but its somewhat of a mistake to assume that this identical
 to what it would be for expressing a particular idea in a certain specific
 way.
 Its possible that, to express the idea, the only sentences that one could
 ever possibly use would have 
\begin_inset Formula $P(w_{l},w_{r})$
\end_inset

 that strongly deviate from a large-corpus average.
 Unfortunately, there is no easy way of knowing what this sentence-specific
 
\begin_inset Formula $P(w_{l},w_{r})$
\end_inset

 is.
 So, instead we make the uniform distribution assumption, that they're all
 the same, and thus get eqn 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:MST entropy"

\end_inset

 instead of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:wrong MST entropy"

\end_inset

.
 Does Yuret ever make this argument himself? Dunno.
\end_layout

\begin_layout Standard
A supporting argument is that we also ignored 3,4,5-point interactions as
 well.
 Which brings us to the next point: why should we expect a link-parse to
 work better than an MST parse? Because Yuret-MST ignores the valence of
 words, whereas the disjuncts don't! The disjuncts provide a better, more
 accurate way of capturing valency!
\end_layout

\begin_layout Subsection*
Entity Extraction
\end_layout

\begin_layout Standard
See Oren Etzioni, Michael Cafarella, Doug Downey, Ana-Maria Popescu Tal
 Shaked, Stephen Soderland, Daniel S.
 Weld, and Alexander Yates (2005) 
\begin_inset Quotes eld
\end_inset

Unsupervised Named-Entity Extraction from the Web: An Experimental Study
\begin_inset Quotes erd
\end_inset

.
 So: KNOWITALL utilizes a set of eight domain-independent extraction patterns
 to generate candidate facts.1 For example, the generic pattern “NP1 such
 as NPList2” ...
 
\begin_inset Quotes eld
\end_inset

cities such as Paris,...
\begin_inset Quotes erd
\end_inset


\begin_inset Quotes erd
\end_inset

 Of course, this is not really unsupervised, since it uses human-generated
 search patterns (
\begin_inset Quotes eld
\end_inset

such as
\begin_inset Quotes erd
\end_inset

) and also applies constraints (the targets must be proper nouns, which
 is not a-priori known).
\end_layout

\begin_layout Section*
3 March 2014
\end_layout

\begin_layout Standard
Start again, after long distraction.
\end_layout

\begin_layout Subsection*
Finding patterns
\end_layout

\begin_layout Standard
To problem.
 Consider an alphabet of 
\begin_inset Formula $N=5$
\end_inset

 letters, 
\begin_inset Formula $\alpha=\{A,B,C,D,E\}$
\end_inset

 and a corpus built from those letters.
 The five letters occur with probability 
\begin_inset Formula $p(w)$
\end_inset

 with 
\begin_inset Formula $w\in\alpha$
\end_inset

.
 Assume the corpus consists entirely of pairs AB, CB and DE, each occurring
 equally often: so: 
\begin_inset Formula $p(A,B)=p(C,B)=p(D,E)=1/3$
\end_inset

 .
 From this, we can reconstruct that 
\begin_inset Formula $p(A)=p(C)=p(D)=p(E)=1/6$
\end_inset

 and 
\begin_inset Formula $p(B)=1/3$
\end_inset

.
 This follows because the corpus can be reduced to 
\begin_inset Formula $\{AB,CB,DE\}$
\end_inset

, so A occurs 1 out of 6 times, B two out of 6 times, etc.
 The total single-letter entropy is thus
\begin_inset Formula 
\begin{align*}
h_{SING} & =-\sum_{w\in\alpha}p(w)\log_{2}p(w)\\
 & =-\frac{4}{6}\log_{2}\frac{1}{6}-\frac{1}{3}\log_{2}\frac{1}{3}\\
 & =\frac{2}{3}-\log_{2}\frac{1}{3}=2.25163
\end{align*}

\end_inset

By contrast, in a random 2-letter corpus, we expect all possible letter
 pairs to occur equally often, i.e.
 
\begin_inset Formula $p(w)=1/5$
\end_inset

, which would result in 
\begin_inset Formula $h_{RAND}=-\log_{2}1/5=2.321928$
\end_inset

 and so we see that the total entropy for this corpus is less than the random
 corpus.
\end_layout

\begin_layout Standard
The total double-word entropy is
\begin_inset Formula 
\begin{align*}
h_{PAIR} & =-\sum_{w_{1},w_{2}\in\alpha}p(w_{1},w_{2})\log_{2}p(w_{1},w_{2})\\
 & =-\log_{2}\frac{1}{3}=1.5849625
\end{align*}

\end_inset

Compare this to 
\begin_inset Formula $h_{PR-RAND}=-\log_{2}1/25=4.643856$
\end_inset

 for the random 2-letter corpus.
 The pair-entropy is sharply lower.
\end_layout

\begin_layout Standard
What do we know about mutual information? We can also deduce that 
\begin_inset Formula $p(*,B)=2/3$
\end_inset

, 
\begin_inset Formula $p(A,*)=1/3$
\end_inset

 and so 
\begin_inset Formula 
\begin{align*}
MI(A,B) & =\log_{2}p(A,B)-\log_{2}p(A,*)-\log_{2}p(*,B)\\
 & =\log_{2}3/2=0.585
\end{align*}

\end_inset

and likewise 
\begin_inset Formula $MI(C,B)=MI(A,B)$
\end_inset

 while 
\begin_inset Formula $MI(D,E)=\log_{2}3=1.585$
\end_inset

.
 
\end_layout

\begin_layout Standard
By contrast, in a random 2-letter corpus, we expect all possible letter
 pairs to occur equally often, i.e.
 
\begin_inset Formula $p(w_{1},w_{2})=1/25$
\end_inset

, which would result in an 
\begin_inset Formula $MI(w_{1},w_{2})=\log_{2}1=0$
\end_inset

 for all word pairs.
 
\end_layout

\begin_layout Standard
Given this corpus, we wish to deduce the following answer: there is a cluster
 
\begin_inset Formula $\gamma=\{A,C\}$
\end_inset

 and two link relations 
\begin_inset Formula $R(\gamma,B)$
\end_inset

 and 
\begin_inset Formula $R(D,E)$
\end_inset

 occuring with probabilities 
\begin_inset Formula $p(\gamma,B)=p(A,B)+p(C,B)=2/3$
\end_inset

 and 
\begin_inset Formula $p(D,E)=1/3$
\end_inset

.
 Note that 
\begin_inset Formula $p(\gamma,*)=p(A,*)+p(C,*)=2/3$
\end_inset

 so that
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
MI(\gamma,B) & =\log_{2}p(\gamma,B)-\log_{2}p(\gamma,*)-\log_{2}p(*,B)\\
 & =\log_{2}3/2=0.585
\end{align*}

\end_inset

So how do we deduce this?
\end_layout

\begin_layout Standard
Well, consider the reduced space, with 
\begin_inset Formula $N=4$
\end_inset

 letters: 
\begin_inset Formula $\beta=\{\gamma,B,D,E\}$
\end_inset

.
 In this space, only two pairs are observed in the corpus, 
\begin_inset Formula $\gamma B$
\end_inset

 and 
\begin_inset Formula $DE$
\end_inset

 with probabilities as above.
 The single-letter probabilities are 
\begin_inset Formula $p(D)=p(E)=1/6$
\end_inset

 and 
\begin_inset Formula $p(\gamma)=p(B)=1/3$
\end_inset

.
 The single-letter entropy is
\begin_inset Formula 
\begin{align*}
h_{SING}^{red} & =-\sum_{w\in\beta}p(w)\log_{2}p(w)\\
 & =-\frac{2}{6}\log_{2}\frac{1}{6}-\frac{2}{3}\log_{2}\frac{1}{3}\\
 & =\frac{1}{3}-\log_{2}\frac{1}{3}=1.9182958
\end{align*}

\end_inset

This can be compared to the entropy of the random 4-word corpus: 
\begin_inset Formula $h_{RAND}^{red}=-\log_{2}1/4=2$
\end_inset

.
 Note that 
\begin_inset Formula 
\[
h_{RAND}^{red}-h_{SING}^{red}=0.081704>0.070298=h_{RAND}-h_{SING}
\]

\end_inset

In other words, the reduced corpus shows more order than the comparable
 unreduced corpus! Interesting! The above can be written as: 
\begin_inset Formula 
\[
h_{SING}-h_{SING}^{red}=0.333334>0.321928=h_{RAND}-h_{RAND}^{red}
\]

\end_inset


\end_layout

\begin_layout Standard
What about the reduced pair entropy? For this case, we have
\begin_inset Formula 
\begin{align*}
h_{PAIR}^{red} & =-\sum_{w_{1},w_{2}\in\beta}p(w_{1},w_{2})\log_{2}p(w_{1},w_{2})\\
 & =-\frac{2}{3}\log_{2}\frac{2}{3}-\frac{1}{3}\log_{2}\frac{1}{3}\\
 & =-\frac{2}{3}-\log_{2}\frac{1}{3}=0.9182958
\end{align*}

\end_inset

which can be compared to the random-pair entropy of 
\begin_inset Formula $h_{PR-RAND}^{red}=-\log_{2}1/16=4$
\end_inset

.
 The comparable reduction is 
\begin_inset Formula 
\[
h_{PR-RAND}^{red}-h_{PAIR}^{red}=3.081704>3.0588935=h_{PR-RAND}-h_{PAIR}
\]

\end_inset

So again, this wins, but not by a lot.
 Re-ordering, this can be written as:
\begin_inset Formula 
\[
h_{PAIR}-h_{PAIR}^{red}=0.6666667>0.643856=h_{PR-RAND}-h_{PR-RAND}^{red}
\]

\end_inset


\end_layout

\begin_layout Standard
So we seem to have two ways of winning: reducing the overall entropy, for
 for single letters, and for pairs, and also finding reductions that are
 strong, even compared to the reduced vocab.
\end_layout

\begin_layout Subsection*
Reductio ad absurdum? No.
\end_layout

\begin_layout Standard
What if we continue on this path, and (incorectly) reduce to 
\begin_inset Formula $N=3$
\end_inset

 letters, with 
\begin_inset Formula $\delta=\{\gamma,\eta,D\}$
\end_inset

 where 
\begin_inset Formula $\eta=\{B,E\}$
\end_inset

? Then 
\begin_inset Formula $p(\eta)=p(B)+p(E)=1/2$
\end_inset

 
\begin_inset Formula 
\begin{align*}
h_{SING}^{rr} & =-\sum_{w\in\delta}p(w)\log_{2}p(w)\\
 & =-\frac{1}{6}\log_{2}\frac{1}{6}-\frac{1}{3}\log_{2}\frac{1}{3}-\frac{1}{2}\log_{2}\frac{1}{2}\\
 & =\frac{2}{3}-\frac{1}{2}\log_{2}\frac{1}{3}=1.4591479
\end{align*}

\end_inset

and the reduction inequality is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
h_{SING}^{red}-h_{SING}^{rr}=0.4591479>0.4150375=h_{RAND}^{red}-h_{RAND}^{rr}
\]

\end_inset

So this inequality allows an inappropriate reduction to take place.
 For the pairs, 
\begin_inset Formula $p(\gamma,\eta)=p(\gamma,B)=2/3$
\end_inset

 and 
\begin_inset Formula $p(D,\eta)=p(D,E)=1/3$
\end_inset

 and everything else being zero.
 Thus one gets:
\begin_inset Formula 
\begin{align*}
h_{PAIR}^{rr} & =-\sum_{w_{1},w_{2}\in\delta}p(w_{1},w_{2})\log_{2}p(w_{1},w_{2})\\
 & =-\frac{2}{3}\log_{2}\frac{2}{3}-\frac{1}{3}\log_{2}\frac{1}{3}\\
 & =-\frac{2}{3}-\log_{2}\frac{1}{3}=0.9182958
\end{align*}

\end_inset

so that 
\begin_inset Formula 
\[
h_{PAIR}^{red}-h_{PAIR}^{rr}=0\ngtr0.830075=h_{PR-RAND}^{red}-h_{PR-RAND}^{rr}
\]

\end_inset

Here, nothing is gained, so the pair inequality blocks the inappropriate
 reduction.
 Consider a differnt inappropraite reduction to 
\begin_inset Formula $N=3$
\end_inset

: let 
\begin_inset Formula $\epsilon=\{\zeta,B,E\}$
\end_inset

 with 
\begin_inset Formula $\zeta=\{D,\gamma\}$
\end_inset

 Then the pair probabilities are 
\begin_inset Formula $p(\zeta,B)=p(\gamma,B)=2/3$
\end_inset

 and 
\begin_inset Formula $p(\zeta,E)=p(D,E)=1/3$
\end_inset

 and again, there is no entropy reduction.
 The other groupings look to be equally ineffective.
\end_layout

\begin_layout Subsection*
Finding Patterns, General Formula
\end_layout

\begin_layout Standard
OK, recast the above section for the (semi-)general case of word-pairs (not
 structures in general).
 So, given a vocabulary of 
\begin_inset Formula $N$
\end_inset

 words, we have 
\begin_inset Formula $h_{RAND}=-\log_{2}\frac{1}{N}=\log_{2}N$
\end_inset

 and 
\begin_inset Formula $h_{RAND}^{red}=\log_{2}(N-1)$
\end_inset

 so that for large 
\begin_inset Formula $N,$
\end_inset

 
\begin_inset Formula $h_{RAND}-h_{RAND}^{red}=\log_{2}N/(N-1)=\log_{2}(1+1/(N-1))\approx1/N$
\end_inset

 and so we have a word-combine winner if we can combine words A and C into
 a cluster 
\begin_inset Formula $\gamma=\{A,C\}$
\end_inset

 such that 
\begin_inset Formula 
\begin{align*}
\frac{1}{N} & \lesssim h_{SING}-h_{SING}^{red}\\
 & =-\sum_{w\in\alpha}p(w)\log_{2}p(w)+\sum_{w\in\beta}p(w)\log_{2}p(w)\\
 & =-p(A)\log_{2}p(A)-p(C)\log_{2}p(C)+p(\gamma)\log_{2}p(\gamma)\\
 & =p(A)\log_{2}\left(1+\frac{p(C)}{p(A)}\right)+p(C)\log_{2}\left(1+\frac{p(A)}{p(C)}\right)
\end{align*}

\end_inset

where 
\begin_inset Formula $p(\gamma)=p(A)+p(C)$
\end_inset

.
 What's not clear: is this inequality 
\emph on
ever
\emph default
 broken? Or does it always hold?
\end_layout

\begin_layout Standard
For pairs, its clear that 
\begin_inset Formula $h_{PR-RAND}-h_{PR-RAND}^{red}\approx2/N$
\end_inset

 which follows as above, given that 
\begin_inset Formula $h_{PR-RAND}=2\log_{2}N$
\end_inset

, etc.
 The corresponding inequality is now
\begin_inset Formula 
\begin{align*}
\frac{2}{N}\lesssim h_{PAIR}-h_{PAIR}^{red}\\
=-\sum_{w_{1},w_{2}\in\alpha} & p(w_{1},w_{2})\log_{2}p(w_{1},w_{2})+\sum_{w_{1},w_{2}\in\beta}p(w_{1},w_{2})\log_{2}p(w_{1},w_{2})\\
=-\sum_{w\in\alpha\backslash A,C} & \left[p(A,w)\log_{2}p(A,w)+p(C,w)\log_{2}p(C,w)-p(\gamma,w)\log_{2}p(\gamma,w)\right]\\
-\sum_{w\in\alpha\backslash A,C} & \left[p(w,A)\log_{2}p(w,A)+p(w,C)\log_{2}p(w,C)-p(w,\gamma)\log_{2}p(w,\gamma)\right]\\
 & -p(A,A)\log_{2}p(A,A)-p(C,A)\log_{2}p(C,A)+p(\gamma)\log_{2}p(\gamma)\\
 & -p(A,C)\log_{2}p(A,C)-p(C,C)\log_{2}p(C,C)
\end{align*}

\end_inset

So...
\end_layout

\begin_layout Section*
The End 
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "lang"
options "alpha"

\end_inset


\end_layout

\end_body
\end_document
